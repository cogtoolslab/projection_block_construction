{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEBUG CODE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this also takes care of all the imports\n",
    "import analysis_helper\n",
    "import analysis_graphs\n",
    "from analysis_helper import *\n",
    "from analysis_graphs import *\n",
    "reload(analysis_helper)\n",
    "reload(analysis_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEBUG CODE END**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block construction agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main analysis file for the [block construction task](https://github.com/cogtoolslab/block_construction). \n",
    "\n",
    "The data should be loaded in by a dataframe produced by experiment_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this also takes care of all the imports\n",
    "from analysis_helper import *\n",
    "from analysis_graphs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the results of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = ['../beam_search.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = ['../search_test.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T08:47:18.277305Z",
     "start_time": "2020-07-11T08:47:15.386871Z"
    }
   },
   "outputs": [],
   "source": [
    "#load all experiments as one dataframe\n",
    "df = pd.concat([pd.read_pickle(l) for l in df_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T08:47:50.928290Z",
     "start_time": "2020-07-11T08:47:50.608334Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have data for the following agents and on the following silhouettes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_short_agent_names(df['agent'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[short_world_name(w) for w in df['world'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO plot nice view of worlds used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only want to look at results for a particular silhouette or agent, the following syntax can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['world'].str.contains('int_struct_15')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview over agents\n",
    "All agents use pure F1 score to judge the value of intermediate states.\n",
    "\n",
    "| Agent |  | Parameter |\n",
    "|:--|:--|:--|\n",
    "| Random (special case of BFS) | Randomly chooses a legal action. | *None* |\n",
    "| Breadth first search | Agent performs breadth first search on the tree of all possible actions and chooses the sequence of actions that has the highest average reward over the next *planning depth* steps | Horizon: how many steps in advance is the tree of possible actions searched? |\n",
    "| MCTS | Implements Monte Carlo Tree Search | Horizon: the number of rollouts per run |\n",
    "| Naive Q learning | Implements naive Q learning with an epsilon-greedy exploration policy | Maximum number of episodes: how many episodes per run |\n",
    "| A* search | Implements A* search algorithm. Runs until winning state is found or an upper limit of steps is reached. | *None* |\n",
    "| Beam search | Implements beam search: searches tree of possible action, but only keeps the best *beam size* actions at each iteration | Beam size: the number of states kept under consideration at every step |\n",
    "\n",
    "### Glossary\n",
    "\n",
    "**Run**: training (if applicable) and running one agent on one particular silhouette.\n",
    "\n",
    "**Silhouette**: the particular outline (and set of baseblocks) that the agent has to reconstruct.\n",
    "\n",
    "**State**: state of the blockworld environment consisting of the blocks that have already been placed in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of perfect reconstruction per agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often does an agent succeed in getting a perfect reconstruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_win_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T13:26:39.006412Z",
     "iopub.status.busy": "2020-08-14T13:26:39.006174Z",
     "iopub.status.idle": "2020-08-14T13:26:39.009427Z",
     "shell.execute_reply": "2020-08-14T13:26:39.008565Z",
     "shell.execute_reply.started": "2020-08-14T13:26:39.006390Z"
    }
   },
   "source": [
    "### F1 score\n",
    "What [F1 score](https://en.wikipedia.org/wiki/F1_score) does the agent achieve? Since F1 score decreases if an agent keeps building after being unable to perfectly recreate the structure, we look at the peak of F1 score for every run. \n",
    "\n",
    "So here is the average peak F1 score per agent conditioned on outcome of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_peak_score_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure kinds\n",
    "In run where the agent fails to achieve perfect reconstruction, what is the reason for the failure?\n",
    "\n",
    "**Full** indicates that no further block can be placed.\n",
    "**Unstable** indicates the structure has collapsed.\n",
    "**Did not finish** means that the agent hasn't finished building either because it terminated or it reached the limit on number of steps (40)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_failure_reason_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greediness\n",
    "Do agents prefer a greedy policy (using large blocks to cover much area) or a conservative policy of using smaller blocks and more steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of steps taken\n",
    "On average, how many steps does an agent take before the end of the run? \n",
    "\n",
    "Looking at the average number of steps for runs with perfect reconstruction (\"win\") tells us whether an agent builds with larger or smaller blocks. \n",
    "\n",
    "Looking at the average number of steps for runs with failed reconstructions (\"fail\") tells whether the failures occur early or late in the process. Since many failures are due to the agent simply filling everything with blocks this number is likely high and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_steps_to_end_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average F1 score during run\n",
    "On average, what is the average F1 score taken over every action up to the peak of F1 score for a particular run? For runs conditioned on perfect reconstructions, what is the growth rate of F1?\n",
    "\n",
    "The higher this number is, the more F1 score is gained early on in the run (ie. a logaritmic looking curve of F1 score). \n",
    "Note that the bars conditioned on winning runs all have a peak F1 score of 1 and are thus directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_avg_area_under_curve_to_peakF1_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average F1 score over time\n",
    "What is the average F1 score over time? \n",
    "\n",
    "Decreasing line indicates the behavior of the agent to keep choosing the least-worst action if a perfect reconstruction is no longer possible.\n",
    "\n",
    "For runs that terminate early, the last F1 score is kept as to not show outliers in the later part of the graph. Thus, a perfect reconstruction at step 8 is counted as a score of 1 for the last 12 steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_mean_F1_over_time_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average block size over time\n",
    "What is the average size of the block placed at a certain step?\n",
    "\n",
    "Note that only runs that aren't finished at a given step are included in the calculation of the mean/std, so later steps might be less informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_avg_blocksize_over_time_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency\n",
    "How consistent are different runs of the same agent on the same silhouette?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality bias\n",
    "Does the agent prefer to place a block on top of or next to the last placed block?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('projection_blocks': conda)",
   "language": "python",
   "name": "python38264bitprojectionblocksconda86e74604eaa74b86af1676acbf4e26f9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
