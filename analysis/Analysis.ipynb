{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEBUG CODE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this also takes care of all the imports\n",
    "import analysis_helper\n",
    "import analysis_graphs\n",
    "from analysis_helper import *\n",
    "from analysis_graphs import *\n",
    "reload(analysis_helper)\n",
    "reload(analysis_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEBUG CODE END**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block construction agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main analysis file for the [block construction task](https://github.com/cogtoolslab/block_construction). \n",
    "\n",
    "The data should be loaded in by a dataframe produced by experiment_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up imports\n",
    "import os\n",
    "import sys\n",
    "__file__ = os.getcwd()\n",
    "proj_dir =  os.path.dirname(os.path.realpath(__file__))\n",
    "utils_dir = os.path.join(proj_dir,'utils')\n",
    "sys.path.append(utils_dir)\n",
    "analysis_dir = os.path.join(proj_dir,'analysis')\n",
    "analysis_utils_dir = os.path.join(analysis_dir,'utils')\n",
    "sys.path.append(analysis_utils_dir)\n",
    "agent_dir = os.path.join(proj_dir,'model')\n",
    "sys.path.append(agent_dir)\n",
    "agent_util_dir = os.path.join(agent_dir,'utils')\n",
    "sys.path.append(agent_util_dir)\n",
    "experiments_dir = os.path.join(proj_dir,'experiments')\n",
    "sys.path.append(experiments_dir)\n",
    "df_dir = os.path.join(proj_dir,'results/dataframes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_helper import *\n",
    "from analysis_graphs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,7)\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the results of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = ['exp_runner_test.pkl'] #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = ['construction_paper_agent.pkl',\n",
    "           'breadth_search.pkl'] #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = ['construction_paper_agent.pkl',\n",
    "           'breadth_search_1_and_3.pkl'] #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = ['breadth_search.pkl'] #for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = ['Astar_search.pkl',\n",
    "           'breadth_search.pkl',\n",
    "           'beam_search.pkl',\n",
    "           'MCTS.pkl',\n",
    "           'Naive_Q_search.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T08:47:18.277305Z",
     "start_time": "2020-07-11T08:47:15.386871Z"
    }
   },
   "outputs": [],
   "source": [
    "#load all experiments as one dataframe\n",
    "df = pd.concat([pd.read_pickle(os.path.join(df_dir,l)) for l in df_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precompute F1 scores for every row\n",
    "fill_F1(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precompute which rows in the dataframe are the last row of a run\n",
    "fill_final_row(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T08:47:50.928290Z",
     "start_time": "2020-07-11T08:47:50.608334Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have data for the following agents, agent configurations and on the following silhouettes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['agent'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['agent_attributes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['world'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graphical illustration of the silhouettes used in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustrate_worlds(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview over agents\n",
    "All agents use pure F1 score to judge the value of intermediate states.\n",
    "\n",
    "| Agent |  | Parameter |\n",
    "|:--|:--|:--|\n",
    "| Random (special case of BFS) | Randomly chooses a legal action. | *None* |\n",
    "| Breadth first search | Agent performs breadth first search on the tree of all possible actions and chooses the sequence of actions that has the highest average reward over the next *planning depth* steps | Horizon: how many steps in advance is the tree of possible actions searched? |\n",
    "| MCTS | Implements Monte Carlo Tree Search | Horizon: the number of rollouts per run |\n",
    "| Naive Q learning | Implements naive Q learning with an epsilon-greedy exploration policy | Maximum number of episodes: how many episodes per run |\n",
    "| A* search | Implements A* search algorithm. Runs until winning state is found or an upper limit of steps is reached. Is determininistic.| *None* |\n",
    "| Beam search | Implements beam search: searches tree of possible action, but only keeps the best *beam size* actions at each iteration. Is determininistic. | Beam size: the number of states kept under consideration at every step |\n",
    "| Construction paper agent | Implements the construction paper agent: it occludes part of the silhuouette, uses a lower level agent to build the remainder, then 'slides up' the occluder and builds again... | Lower level agent: the lower level planning agent together with its parameters |\n",
    "\n",
    "### Glossary\n",
    "\n",
    "**Run**: training (if applicable) and running one agent on one particular silhouette.\n",
    "\n",
    "**Silhouette**: the particular outline (and set of baseblocks) that the agent has to reconstruct.\n",
    "\n",
    "**State**: state of the blockworld environment consisting of the blocks that have already been placed in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of perfect reconstruction per agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often does an agent succeed in getting a perfect reconstruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_win_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of perfect reconstruction per agent over silhouettes\n",
    "How often does an agent achieve a perfect reconstruction in a particular world?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_win_per_agent_over_worlds(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T13:26:39.006412Z",
     "iopub.status.busy": "2020-08-14T13:26:39.006174Z",
     "iopub.status.idle": "2020-08-14T13:26:39.009427Z",
     "shell.execute_reply": "2020-08-14T13:26:39.008565Z",
     "shell.execute_reply.started": "2020-08-14T13:26:39.006390Z"
    }
   },
   "source": [
    "### F1 score\n",
    "What [F1 score](https://en.wikipedia.org/wiki/F1_score) does the agent achieve? Since F1 score decreases if an agent keeps building after being unable to perfectly recreate the structure, we look at the peak of F1 score for every run. \n",
    "\n",
    "So here is the average peak F1 score per agent conditioned on outcome of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_peak_score_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score over silhouettes\n",
    "What is the peak F1 score for a particular silhouette?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_peak_F1_per_agent_over_worlds(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure kinds\n",
    "In run where the agent fails to achieve perfect reconstruction, what is the reason for the failure?\n",
    "\n",
    "**Full** indicates that no further block can be placed.\n",
    "**Unstable** indicates the structure has collapsed.\n",
    "**Did not finish** means that the agent hasn't finished building either because it terminated or it reached the limit on number of steps (40). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_failure_reason_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure kinds over silhouettes\n",
    "In kinds of failure does a certain agent on a certain silhouette tend to make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_failure_reason_per_agent_over_worlds(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps\n",
    "To get a qualitative sense of what the agents are doing, let's look at the heatmap of all runs of an agent on a particular silhouette at the moment of peak F1. Looking at final states will include many runs in which the agent has simply filled the entire area. \n",
    "Brighter colors indicate higher average occupancy of a cell. The target is shown on the left.\n",
    "\n",
    "Note that if the final block placed was unstable it is still included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps_at_peak_per_agent_over_world(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greediness\n",
    "Do agents prefer a greedy policy (using large blocks to cover much area) or a conservative policy of using smaller blocks and more steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of steps taken\n",
    "On average, how many steps does an agent take before the end of the run? \n",
    "\n",
    "Looking at the average number of steps for runs with perfect reconstruction (\"win\") tells us whether an agent builds with larger or smaller blocks. \n",
    "\n",
    "Looking at the average number of steps for runs with failed reconstructions (\"fail\") tells whether the failures occur early or late in the process. Since many failures are due to the agent simply filling everything with blocks this number is likely high and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_steps_to_end_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth rate of F1 score\n",
    "On average, what is the average F1 score taken over every action up to the peak of F1 score for a particular run? For runs conditioned on perfect reconstructions, what is the growth rate of F1?\n",
    "\n",
    "The higher this number is, the more F1 score is gained early on in the run (ie. a logaritmic looking curve of F1 score). \n",
    "Note that the bars conditioned on winning runs all have a peak F1 score of 1 and are thus directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_avg_area_under_curve_to_peakF1_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average F1 score over time\n",
    "What is the average F1 score over time? \n",
    "\n",
    "Decreasing line indicates the behavior of the agent to keep choosing the least-worst action if a perfect reconstruction is no longer possible.\n",
    "\n",
    "For runs that terminate early, the last F1 score is kept as to not show outliers in the later part of the graph. Thus, a perfect reconstruction at step 8 is counted as a score of 1 for the last 12 steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_mean_F1_over_time_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average block size over time\n",
    "What is the average size of the block placed at a certain step?\n",
    "\n",
    "Note that only runs that aren't finished at a given step are included in the calculation of the mean/std, so later steps might be less informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_avg_blocksize_over_time_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency\n",
    "### Average pairwise Euclidean distance of block placements\n",
    "How consistent are different runs of the same agent on the same silhouette?\n",
    "\n",
    "Here, we measure the average pairwise distance for an agent across runs on the same silhouette. A lower score indicates higher similarity. A score of 0 indicates that all runs were identical—this occurs when the agent is deterministic.\n",
    "\n",
    ">For any pair of action sequences, we define the “raw action dissimilarity” as the mean Euclidean distance between corresponding pairs of [x, y, w, h] action vectors. When two sequences are of different lengths, we evaluate this metric over the first k actions in both, where k represents the length of the shorter sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This scales exponentially and takes a really long time\n",
    "mean_pairwise_raw_euclidean_distance_between_runs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory graph\n",
    "The trajectory graph (adopted from [block_construction](https://github.com/cogtoolslab/block_construction)) shows the path all runs from a single agent on a particular value take through state space. The y axis orders states by F1 score. The size of nodes indicates how common a certain state is. The color indicates whether the coloured edge ends in failure (red) or at least one perfect reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_per_agent_over_world(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality bias\n",
    "### Proportion of local block placements\n",
    "Does the agent prefer to place a block on top of or next to the last placed block?\n",
    "\n",
    "The score is calculated by looking at what percentage of blocks placed during a run touch (either top/bottom or sides, not corners) the block placed immediately before. \n",
    "A score of 1 indicates that all blocks were placed on the last one, a score of 0 indicates that the agent switched to a different location to build at every opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_touching_last_block_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning cost\n",
    "### How many states are evaluated during planning?\n",
    "How many states are evaluated during planning? This is a proxy for how expensive and effective the planning of an agent is. \n",
    "\n",
    "Low scores for the runs conditioned on perfect reconstructions indicate that often when a solution can be found, it can be found quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_avg_states_evaluated_per_agent(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('projection_blocks': conda)",
   "language": "python",
   "name": "python38264bitprojectionblocksconda86e74604eaa74b86af1676acbf4e26f9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
