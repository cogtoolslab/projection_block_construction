{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block construction agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main general analysis file for the [block construction task](https://github.com/cogtoolslab/block_construction). \n",
    "\n",
    "The data should be loaded in by a dataframe produced by experiment_runner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up imports\n",
    "import os\n",
    "import sys\n",
    "__file__ = os.getcwd()\n",
    "proj_dir =  os.path.dirname(os.path.realpath(__file__))\n",
    "sys.path.append(proj_dir)\n",
    "utils_dir = os.path.join(proj_dir,'utils')\n",
    "sys.path.append(utils_dir)\n",
    "analysis_dir = os.path.join(proj_dir,'analysis')\n",
    "analysis_utils_dir = os.path.join(analysis_dir,'utils')\n",
    "sys.path.append(analysis_utils_dir)\n",
    "agent_dir = os.path.join(proj_dir,'model')\n",
    "sys.path.append(agent_dir)\n",
    "agent_util_dir = os.path.join(agent_dir,'utils')\n",
    "sys.path.append(agent_util_dir)\n",
    "experiments_dir = os.path.join(proj_dir,'experiments')\n",
    "sys.path.append(experiments_dir)\n",
    "df_dir = os.path.join(proj_dir,'results/dataframes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.utils.analysis_helper import *\n",
    "from analysis.utils.analysis_graphs import *\n",
    "from analysis.utils.analysis_figures import *\n",
    "import utils.blockworld as bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.utils.decomposition_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inline plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (40,7)\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 20)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.min_rows', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the results of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = ['subgoal planning full BFS3.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T08:47:18.277305Z",
     "start_time": "2020-07-11T08:47:15.386871Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load all experiments as one dataframe\n",
    "df = pd.concat([pd.read_pickle(os.path.join(df_dir,l)) for l in df_paths])\n",
    "print(\"Loaded dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "preprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider saving the costly preprocessing\n",
    "df.to_pickle(os.path.join(df_dir,str(df_paths)+\"_preprocessed.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-11T08:47:50.928290Z",
     "start_time": "2020-07-11T08:47:50.608334Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have data for the following agents, agent configurations and on the following silhouettes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(smart_short_agent_names(df['agent_attributes'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['agent_attributes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['agent_label','decomposed_silhouette']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "type(df.iloc[7]['decomposed_silhouette'][0]['decomposition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['world'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add human readable labels for nice plotting if the smart labels aren't good enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agent_attributes_string'] = df['agent_attributes'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce dict for agent naming\n",
    "agents = df['agent_attributes_string'].unique()\n",
    "agent_labels = dict(zip(list(agents),[\"XXX\"]*len(agents)))\n",
    "#now manually add names and save back\n",
    "agent_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save back into agent_labels (just copy and paste...)\n",
    "agent_labels = {\"{'agent_type': 'BFS_Agent', 'scoring_function': 'F1score', 'horizon': 1, 'scoring_type': 'Average'}\": '1',\n",
    " \"{'agent_type': 'BFS_Agent', 'scoring_function': 'random_scoring', 'horizon': 1, 'scoring_type': 'Average'}\": 'random',\n",
    " \"{'agent_type': 'BFS_Agent', 'scoring_function': 'F1score', 'horizon': 2, 'scoring_type': 'Average'}\": '2',\n",
    " \"{'agent_type': 'BFS_Agent', 'scoring_function': 'F1score', 'horizon': 3, 'scoring_type': 'Average'}\": '3',\n",
    " \"{'agent_type': 'BFS_Agent', 'scoring_function': 'F1score', 'horizon': 4, 'scoring_type': 'Average'}\": '4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to dataframe\n",
    "df['agent_label'] = df['agent_attributes_string'].replace(agent_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only include big silhouettes\n",
    "df = df[df['world'].str.contains('int_struct')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graphical illustration of the silhouettes used in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illustrate_worlds(df.sort_values('world'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview over agents\n",
    "All agents use pure F1 score to judge the value of intermediate states.\n",
    "\n",
    "| Agent |  | Parameters |\n",
    "|:--|:--|:--|\n",
    "| Random (special case of BFS) | Randomly chooses a legal action. | *None* |\n",
    "| Breadth first search | Agent performs breadth first search on the tree of all possible actions and chooses the sequence of actions that has the highest average reward over the next *planning depth* steps | Horizon: how many steps in advance is the tree of possible actions searched? |\n",
    "| MCTS | Implements Monte Carlo Tree Search | Horizon: the number of rollouts per run |\n",
    "| Naive Q learning | Implements naive Q learning with an epsilon-greedy exploration policy | Maximum number of episodes: how many episodes per run |\n",
    "| A* search | Implements A* search algorithm. Runs until winning state is found or an upper limit of steps is reached. Is determininistic.| *None* |\n",
    "| Beam search | Implements beam search: searches tree of possible action, but only keeps the best *beam size* actions at each iteration. Is determininistic. | Beam size: the number of states kept under consideration at every step |\n",
    "| Construction paper agent | Implements the construction paper agent: it occludes part of the silhuouette, uses a lower level agent to build the remainder, then 'slides up' the occluder and builds again... | Lower level agent: the lower level planning agent together with its parameters |\n",
    "| Subgoal planning agent | Implements planning over `lookahead` many subgoals akin to Correa and Ho and then acts the first one, plans again.| Lookahead: how many subgoals to plan into the Future. \\ c_weight: how to weigh cost against reward |\n",
    "| Full Subgoal planning agent | Implements planning over all possible subgoals akin to Correa and Ho and then acts all of them.| c_weight: how to weigh cost against reward |\n",
    "\n",
    "### Glossary\n",
    "\n",
    "**Run**: training (if applicable) and running one agent on one particular silhouette.\n",
    "\n",
    "**Silhouette**: the particular outline (and set of baseblocks) that the agent has to reconstruct.\n",
    "\n",
    "**State**: state of the blockworld environment consisting of the blocks that have already been placed in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for random run\n",
    "agent_type = 'Subgoal_Planning_Agent'\n",
    "lookahead = 1\n",
    "c_weight = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in range(10):\n",
    "    a_w_df = df.query(\"lookeahead == @lookahead and c_weight == @c_weight and world == 'int_struct_11' and agent_type == @agent_type\")\n",
    "    r_ID = random.choice(a_w_df['run_ID'].unique())\n",
    "    random_run = a_w_df[a_w_df['run_ID']==r_ID]\n",
    "    print(random_run.tail(1)['world_failure_reason'].item())\n",
    "    build_animation(random_run,agent_type+\" la: \"+str(lookahead)+\" cw: \"+str(c_weight)+\" nr. \"+str(c+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success vs effiency\n",
    "How does the success of an agent relate to it's computational efficiency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_success_cost(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of perfect reconstruction per agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How often does an agent succeed in getting a perfect reconstruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_win_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of perfect reconstruction per agent over silhouettes\n",
    "How often does an agent achieve a perfect reconstruction in a particular world?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_win_per_agent_over_worlds(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T13:26:39.006412Z",
     "iopub.status.busy": "2020-08-14T13:26:39.006174Z",
     "iopub.status.idle": "2020-08-14T13:26:39.009427Z",
     "shell.execute_reply": "2020-08-14T13:26:39.008565Z",
     "shell.execute_reply.started": "2020-08-14T13:26:39.006390Z"
    }
   },
   "source": [
    "### F1 score\n",
    "What [F1 score](https://en.wikipedia.org/wiki/F1_score) does the agent achieve? Since F1 score decreases if an agent keeps building after being unable to perfectly recreate the structure, we look at the peak of F1 score for every run. \n",
    "\n",
    "So here is the average peak F1 score per agent conditioned on outcome of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_peak_score_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score over silhouettes\n",
    "What is the peak F1 score for a particular silhouette?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_peak_F1_per_agent_over_worlds(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure kinds\n",
    "In run where the agent fails to achieve perfect reconstruction, what is the reason for the failure?\n",
    "\n",
    "**Full** indicates that no further block can be placed.\n",
    "**Unstable** indicates the structure has collapsed.\n",
    "**Did not finish** means that the agent hasn't finished building either because it terminated or it reached the limit on number of steps (40). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_failure_reason_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure kinds over silhouettes\n",
    "In kinds of failure does a certain agent on a certain silhouette tend to make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_failure_reason_per_agent_over_worlds(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holes\n",
    "The most common failure mode is leaving holes in the structure that should be covered with a block, but that the agent can't get to because a block has been above. Here, the measure is defined as the number of grid cells that are in the silhouette, not built on, but the cell right above is built. The higher this number, the more or the larger the holes are that the agent builds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score_per_agent(df,scoring_function=bw.holes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps\n",
    "To get a qualitative sense of what the agents are doing, let's look at the heatmap of all runs of an agent on a particular silhouette at the moment of peak F1. Looking at final states will include many runs in which the agent has simply filled the entire area. \n",
    "Brighter colors indicate higher average occupancy of a cell. The target is shown on the left.\n",
    "\n",
    "Note that if the final block placed was unstable it is still included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "heatmaps_at_peak_per_agent_over_world(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps_per_agent_over_world(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greediness\n",
    "Do agents prefer a greedy policy (using large blocks to cover much area) or a conservative policy of using smaller blocks and more steps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of steps taken\n",
    "On average, how many steps does an agent take before the end of the run? \n",
    "\n",
    "Looking at the average number of steps for runs with perfect reconstruction (\"win\") tells us whether an agent builds with larger or smaller blocks. \n",
    "\n",
    "Looking at the average number of steps for runs with failed reconstructions (\"fail\") tells whether the failures occur early or late in the process. Since many failures are due to the agent simply filling everything with blocks this number is likely high and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_steps_to_end_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growth rate of F1 score\n",
    "On average, what is the average F1 score taken over every action up to the peak of F1 score for a particular run? For runs conditioned on perfect reconstructions, what is the growth rate of F1?\n",
    "\n",
    "The higher this number is, the more F1 score is gained early on in the run (ie. a logaritmic looking curve of F1 score). \n",
    "Note that the bars conditioned on winning runs all have a peak F1 score of 1 and are thus directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_avg_area_under_curve_to_peakF1_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average F1 score over time\n",
    "What is the average F1 score over time? \n",
    "\n",
    "Decreasing line indicates the behavior of the agent to keep choosing the least-worst action if a perfect reconstruction is no longer possible.\n",
    "\n",
    "For runs that terminate early, the last F1 score is kept as to not show outliers in the later part of the graph. Thus, a perfect reconstruction at step 8 is counted as a score of 1 for the last 12 steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_mean_F1_over_time_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average block size over time\n",
    "What is the average size of the block placed at a certain step?\n",
    "\n",
    "Note that only runs that aren't finished at a given step are included in the calculation of the mean/std, so later steps might be less informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_avg_blocksize_over_time_per_agent(df.query(\"lookeahead == 1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency\n",
    "### Average pairwise Euclidean distance of block placements\n",
    "How consistent are different runs of the same agent on the same silhouette?\n",
    "\n",
    "Here, we measure the average pairwise distance for an agent across runs on the same silhouette. A lower score indicates higher similarity. A score of 0 indicates that all runs were identical—this occurs when the agent is deterministic.\n",
    "\n",
    ">For any pair of action sequences, we define the “raw action dissimilarity” as the mean Euclidean distance between corresponding pairs of [x, y, w, h] action vectors. When two sequences are of different lengths, we evaluate this metric over the first k actions in both, where k represents the length of the shorter sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This scales exponentially and takes a really long time\n",
    "mean_pairwise_raw_euclidean_distance_between_runs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory graph\n",
    "The trajectory graph (adopted from [block_construction](https://github.com/cogtoolslab/block_construction)) shows the path all runs from a single agent on a particular value take through state space. The y axis orders states by F1 score. The size of nodes indicates how common a certain state is. The color indicates whether the coloured edge ends in failure (red) or at least one perfect reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_per_agent_over_world(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality bias\n",
    "### Proportion of local block placements\n",
    "Does the agent prefer to place a block on top of or next to the last placed block?\n",
    "\n",
    "The score is calculated by looking at what percentage of blocks placed during a run touch (either top/bottom or sides, not corners) the block placed immediately before. \n",
    "A score of 1 indicates that all blocks were placed on the last one, a score of 0 indicates that the agent switched to a different location to build at every opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_touching_last_block_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order\n",
    "Do agents show a bias in which order different parts of the structure are built? To analyze this, here are the heatmaps per silhouette and per agent that show for each cell the average index of the block in that cell. A value of 3.2 would indicate that the block in that cell was placed on average as the 3.2rd block in that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmaps_block_index_per_agent_over_world(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning cost\n",
    "### How many states are evaluated during planning?\n",
    "How many states are evaluated during planning? This is a proxy for how expensive and effective the planning of an agent is. \n",
    "\n",
    "Low scores for the runs conditioned on perfect reconstructions indicate that often when a solution can be found, it can be found quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_avg_states_evaluated_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of tool use\n",
    "What effect does the use of a tool have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise efficiency\n",
    "If we run an agent with and without tools, what is the rate of perfect reconstructions it achieves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually fill the list of agent pairs with (tool, no tool, label)\n",
    "agent_pairs = (\n",
    "    [\n",
    "        (\n",
    "       \"{'agent_type': 'Construction_Paper_Agent', 'decomposition_function': 'random_1_4', 'lower level: agent_type': 'BFS_Agent', 'lower level: scoring_function': 'random_scoring', 'lower level: horizon': 1, 'lower level: scoring_type': 'Final_state', 'lower level: random_seed': None}\",\n",
    "       \"{'agent_type': 'BFS_Agent', 'scoring_function': 'random_scoring', 'horizon': 1, 'scoring_type': 'Final_state'}\",\n",
    "       \"Random\"),\n",
    "       (\n",
    "       \"{'agent_type': 'Construction_Paper_Agent', 'decomposition_function': 'random_1_4', 'lower level: agent_type': 'BFS_Agent', 'lower level: scoring_function': 'silhouette_score', 'lower level: horizon': 1, 'lower level: scoring_type': 'Final_state', 'lower level: random_seed': None}\",\n",
    "       \"{'agent_type': 'BFS_Agent', 'scoring_function': 'silhouette_score', 'horizon': 1, 'scoring_type': 'Final_state'}\",\n",
    "       \"Horizon 1\"),\n",
    "       (\n",
    "       \"{'agent_type': 'Construction_Paper_Agent', 'decomposition_function': 'random_1_4', 'lower level: agent_type': 'BFS_Agent', 'lower level: scoring_function': 'silhouette_score', 'lower level: horizon': 2, 'lower level: scoring_type': 'Final_state', 'lower level: random_seed': None}\",\n",
    "       \"{'agent_type': 'BFS_Agent', 'scoring_function': 'silhouette_score', 'horizon': 2, 'scoring_type': 'Final_state'}\",\n",
    "       \"Horizon 2\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_success_pairs(df,agent_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgoal planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many subgoals are found?\n",
    "This graphs displays the mean number of actual subgoals per run (as in: subgoals passed to the lower level agent, ignoring lookahead) per agent. Note that this only works for agents that act out one subgoal before having to plan again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_num_subgoals_per_agent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the cost of subgoal planning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO subgoal planning bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ratio_successful'] = df['_all_sequences'].apply(lambda x:len([s for s in x if s.complete()])/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['world','all_sequences_planning_cost','lower level: horizon','lower level: scoring_function','ratio_successful']].sort_values(by=['world','lower level: scoring_function','lower level: horizon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = df.query(\"all_sequences_planning_cost == 0\")['_all_sequences'].head(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seq_names'] = df['_chosen_subgoal_sequence'].apply(get_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(x):\n",
    "    try:\n",
    "        return x.names()\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.query(\"c_weight == 0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tools_bc]",
   "language": "python",
   "name": "conda-env-tools_bc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
