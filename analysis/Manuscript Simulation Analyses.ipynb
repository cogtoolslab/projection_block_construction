{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Analyses\n",
    "This notebook contains the code to reproduce analyses related to the computational simulations for the main paper version of Visual Scoping. \n",
    "\n",
    "> What should we expect from following different strategies of taking future costs into account?\n",
    "\n",
    "This notebook is organized according to the topic sentences, which can be found in the tex file of the manuscript.\n",
    "For human data and model fitting, see other notebooks in this folder.\n",
    "\n",
    "Requires:\n",
    "\n",
    "* `.pkl` files generated by `experiment/simulate_agents.*.py` (which in turn need `.pkl` files that hold the cached sequences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up imports\n",
    "import os\n",
    "import sys\n",
    "__file__ = os.getcwd()\n",
    "proj_dir =  os.path.dirname(os.path.realpath(__file__))\n",
    "sys.path.append(proj_dir)\n",
    "utils_dir = os.path.join(proj_dir,'utils')\n",
    "sys.path.append(utils_dir)\n",
    "analysis_dir = os.path.join(proj_dir,'analysis')\n",
    "analysis_utils_dir = os.path.join(analysis_dir,'utils')\n",
    "sys.path.append(analysis_utils_dir)\n",
    "agent_dir = os.path.join(proj_dir,'model')\n",
    "sys.path.append(agent_dir)\n",
    "agent_util_dir = os.path.join(agent_dir,'utils')\n",
    "sys.path.append(agent_util_dir)\n",
    "experiments_dir = os.path.join(proj_dir,'experiments')\n",
    "stim_dir = os.path.join(proj_dir,'stimuli')\n",
    "sys.path.append(stim_dir)\n",
    "sys.path.append(experiments_dir)\n",
    "df_dir = os.path.join(proj_dir,'results/dataframes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Subgoal_Planning_Agent import *\n",
    "import utils.blockworld as bw\n",
    "import utils.blockworld_library as bl\n",
    "from stimuli.tower_generator import TowerGenerator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import sem as sem\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import p_tqdm\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "def str2array(s):\n",
    "    #strip \"array\" and parentheses\n",
    "    s=re.sub('\\[array\\(', '', s.strip())\n",
    "    s=re.sub('\\)]', '', s.strip())\n",
    "    # Remove space after [\n",
    "    s=re.sub('\\[ +', '[', s.strip())\n",
    "    # Replace commas and spaces\n",
    "    s=re.sub('[,\\s]+', ', ', s)\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def str2list(s):\n",
    "    if s is np.nan: return s\n",
    "    #strip \"array\" and parentheses\n",
    "    s=re.sub('\\[array\\(', '', s.strip())\n",
    "    s=re.sub('\\)]', '', s.strip())\n",
    "    # Remove space after [\n",
    "    s=re.sub('\\[ +', '[', s.strip())\n",
    "    # Replace commas and spaces\n",
    "    s=re.sub('[,\\s]+', ', ', s)\n",
    "    return list(ast.literal_eval(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for pd.agg\n",
    "def item(x):\n",
    "    return x.tail(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inline plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot styling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7,7)\n",
    "plt.rcParams.update({'font.size': 26})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "# plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Helvetica']\n",
    "rc('text.latex', preamble=r'\\usepackage{tgheros} \\usepackage{newtxsf} \\renewcommand{\\familydefault}{\\sfdefault} \\usepackage{mathastext}') #sets the font via latex preamble—only way to autoset tick labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 20)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.min_rows', 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Let's load the results of the experiment. Provide the path to a folder containing the dataframes genereated by `simulate_agents.*.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/felixbinder/Cloud/Grad School/Fan Lab/Block Construction/tools_block_construction/results/dataframes/simulated_subgoal_agents__4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "print(\"Got {} csv files\".format(len(csv_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.pkl')]\n",
    "print(\"Got {} pkl files\".format(len(pkl_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load all experiments as one dataframe from CSV\n",
    "# dfs = [pd.read_csv(os.path.join(df_dir,l)) for l in csv_paths]\n",
    "# print(\"Read {} dataframes:\".format(len(dfs)))\n",
    "# # merge dfs\n",
    "# df = pd.concat(dfs)\n",
    "# print(\"Merged dataframes: {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DEVELOPMENT\n",
    "# cut short the file list\n",
    "# pkl_paths = pkl_paths[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all experiments as one dataframe from pickles—this can take a long time and use a lot of memory\n",
    "dfs = [pd.read_pickle(os.path.join(df_dir,l)) for l in tqdm(pkl_paths)]\n",
    "print(\"Read {} dataframes\".format(len(dfs)))\n",
    "# merge dfs\n",
    "df = pd.concat(dfs)\n",
    "print(\"Merged dataframes: {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a few things to add helpful columns and such\n",
    "# use either solution_cost or states_evaluated as cost\n",
    "df['cost'] = np.maximum(df['partial_solution_cost'].fillna(0),\n",
    "                        df['states_evaluated'].fillna(0))\n",
    "# do the same for total cost\n",
    "df['total_cost'] = np.maximum(df['all_sequences_planning_cost'].fillna(\n",
    "    0), df['states_evaluated'].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add world size—note that world size is in pixels, not number of blocks in the reference solution\n",
    "df['world_size'] = df['_world'].apply(lambda x: np.sum(x.full_silhouette > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also want the number of subgoals considered\n",
    "df['all_sequences_count'] = df['_all_subgoal_sequences'].apply(lambda x: len(x) if type(x) is not float else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdf holds final rows for every run\n",
    "fdf = df.groupby('run_ID').agg({\n",
    "        'agent': 'first',\n",
    "        'agent_type': item,\n",
    "        'c_weight': 'first',\n",
    "        'label': 'first',\n",
    "        'world': item,\n",
    "        'action': 'count',\n",
    "        'blockmap': 'last',\n",
    "        'states_evaluated': ['sum', 'mean', sem],\n",
    "        'planning_cost': ['sum', 'mean', sem], \n",
    "        'partial_planning_cost': ['sum', 'mean', sem], # the planning cost of the sequence as far as acted\n",
    "        'partial_solution_cost': ['sum', 'mean', sem],\n",
    "        'solution_cost': ['sum', 'mean', sem],\n",
    "        'all_sequences_planning_cost': ['sum', 'mean', sem],\n",
    "        'all_sequences_count': 'sum',\n",
    "        # 'num_subgoals_acted': ['sum', 'mean', sem],\n",
    "        'perfect': 'last',\n",
    "        'planning_step': 'max',\n",
    "        'cost': ['sum', 'mean', sem],\n",
    "        'total_cost': ['sum', 'mean', sem],\n",
    "        'world_size': item,\n",
    "})\n",
    "\n",
    "#flatten the dataframe to remove multi-index for next groupby\n",
    "fdf.columns = [' '.join(col).strip() for col in fdf.columns.values]\n",
    "fdf.reset_index(inplace=True)\n",
    "# What is the number of blocks used?\n",
    "# fdf['num_blocks'] = fdf['blockmap last'].apply(lambda x: np.max(str2array(x)))\n",
    "#store note order as categorical to ensure sort\n",
    "# fdf['note item'] = pd.Categorical(fdf['note item'],NOTE_ORDER) #restore the order of column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction functions\n",
    "def CI95(data): #this is NOT bootstrapped\n",
    "#     return st.t.interval(alpha=0.95,df=len(data)-1,loc=np.mean(data),scale=st.sem(data))\n",
    "    return tuple(np.percentile(data,[2.5,97.5]))\n",
    "\n",
    "def names(list_names):\n",
    "    if list_names is np.nan: return np.nan\n",
    "    return [g for g in list_names if g is not np.nan]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_over_runs(df, column, stat_function = np.mean, CIs = [2.5,97.5], iterations = 1000, show_tqdm = True):\n",
    "    \"\"\"Bootstrap by choosing individual runs across towers (`run_ID`). \n",
    "    The given df should only contain rows for the relevant algorithm/conditions.\n",
    "    Returns mean and CI of mean.\"\"\"\n",
    "    measurements = np.zeros(iterations)\n",
    "    for i in tqdm(range(iterations), leave=False, disable = not show_tqdm):\n",
    "        #sample with replacement\n",
    "        run = df.sample(frac=1, replace=True)[column]\n",
    "        #save that run\n",
    "        measurements[i] = stat_function(run)\n",
    "    #compute mean and CI over measurements\n",
    "    return np.mean(measurements),np.percentile(measurements, CIs), stat_function(df[column])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(label):\n",
    "    if label == \"No Subgoal\": return \"o\"\n",
    "    if label == \"Myopic\": return \"s\"\n",
    "    if label == \"Lookahead\": return \"^\"\n",
    "    if label == \"Lookahead 2\": return \"v\"\n",
    "    if \"Full Decomp\" in label: return \"d\"\n",
    "    return \"o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colors(label):\n",
    "    if label == \"Myopic\":\n",
    "        return 'limegreen'\n",
    "    if label == \"Lookahead 1\":\n",
    "        return 'cornflowerblue'\n",
    "    if label == \"Lookahead 2\":\n",
    "        return 'darkblue'\n",
    "    if label == \"No Subgoals\":\n",
    "        return 'grey'\n",
    "    if label == \"Full Decomp\":\n",
    "        return 'hotpink'\n",
    "    if label == \"Full Decomp 3\":\n",
    "        return 'hotpink'\n",
    "    if label == \"Full Decomp 4\":\n",
    "        return \"purple\"\n",
    "    # if 'Myopic' in label:\n",
    "    #     return [43/255,108/255,162/255,]\n",
    "    # elif 'Lookahead' in label:\n",
    "    #     return [150/255,43/255,162/255,]\n",
    "    # elif 'Best First' in label:\n",
    "    #     return [42/255,132/255,94/255,]\n",
    "    # else:\n",
    "    #     return [174/255,55/255,4/255,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(fdf, x_axis, y_axis, x_label, y_label, title, x_scale='linear', y_scale='log', legend=True):\n",
    "    plt.figure(figsize=(7, 7))\n",
    "\n",
    "    for label in tqdm(fdf['label first'].unique()):\n",
    "        ag_df = fdf[fdf['label first'] == label]\n",
    "        ag_df = ag_df.sort_values(by=x_axis)\n",
    "        jitter = np.random.normal(0, 0.5, ag_df.shape[0])\n",
    "\n",
    "        plt.scatter(ag_df[x_axis]+jitter, ag_df[y_axis], alpha=0.5, marker=get_shape(label), color=get_colors(label))\n",
    "\n",
    "        means = []\n",
    "        cis = []\n",
    "\n",
    "        for size in ag_df[x_axis].unique():\n",
    "            mean, ci, true_mean = bootstrap_over_runs(ag_df[ag_df[x_axis] == size], y_axis, stat_function=np.mean, show_tqdm=False)\n",
    "            means.append(mean)\n",
    "            cis.append(ci)\n",
    "\n",
    "        plt.plot(ag_df[x_axis].unique(), means, color=get_colors(label), marker=get_shape(label), markersize=10, label=label)\n",
    "        plt.fill_between(ag_df[x_axis].unique(), [c[0] for c in cis], [c[1] for c in cis], alpha=0.2, color=get_colors(label))\n",
    "\n",
    "    plt.yscale(y_scale)\n",
    "    plt.xscale(x_scale)\n",
    "    if legend: \n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    # plt.show()\n",
    "    # return the plot object\n",
    "    return plt.gcf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving these towers by doing action level search alone is hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['label'] == 'No Subgoals'].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean success rate for no subgoals\")\n",
    "mean, ci, true_mean = bootstrap_over_runs(fdf[fdf['label first'] == 'No Subgoals'], 'perfect last', stat_function = np.mean)\n",
    "print(\"True Mean: {}, Bootstraped Mean: {}, CI: {}\".format(true_mean, mean, ci))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Action Level Cost for no subgoals\")\n",
    "mean, ci, true_mean = bootstrap_over_runs(df[df['label'] == 'No Subgoals'], 'cost', stat_function = np.mean)\n",
    "print(\"True Mean: {}, Bootstrapped Mean: {}, CI: {}\".format(true_mean, mean, ci))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation to tower size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smallest and largest worlds cost and success rate\n",
    "# bootstrap\n",
    "print(\"Smallest worlds\")\n",
    "print(\"Smallest size: {}\".format(fdf['world_size item'].min()))\n",
    "mean, ci, true_mean = bootstrap_over_runs(fdf[fdf['world_size item'] == fdf['world_size item'].min()], 'cost sum', stat_function = np.mean)\n",
    "print(\"*Cost sum*: True Mean: {}, Bootstrapped Mean: {}, CI: {}\".format(true_mean, mean, ci))\n",
    "mean, ci, true_mean = bootstrap_over_runs(fdf[fdf['world_size item'] == fdf['world_size item'].min()], 'perfect last', stat_function = np.mean)\n",
    "print(\"*Perfect*: True Mean: {}, Bootstrapped Mean: {}, CI: {}\".format(true_mean, mean, ci))\n",
    "print(\"Largest world\")\n",
    "print(\"Largest size: {}\".format(fdf['world_size item'].max()))\n",
    "mean, ci, true_mean = bootstrap_over_runs(fdf[fdf['world_size item'] == fdf['world_size item'].max()], 'cost sum', stat_function = np.mean)\n",
    "print(\"*Cost sum*: True Mean: {}, Bootstrapped Mean: {}, CI: {}\".format(true_mean, mean, ci))\n",
    "mean, ci, true_mean = bootstrap_over_runs(fdf[fdf['world_size item'] == fdf['world_size item'].max()], 'perfect last', stat_function = np.mean)\n",
    "print(\"*Perfect*: True Mean: {}, Bootstrapped Mean: {}, CI: {}\".format(true_mean, mean, ci))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The use of visual subgoals can greatly reduce the action planning costs over not using subgoals, with full decomposition reducing it most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_summary = pd.DataFrame(columns = ['mean action level cost', 'mean subgoal level cost', 'mean success rate', 'CI action level cost', 'CI subgoal level cost', 'CI success rate'], index = fdf['label first'].unique())\n",
    "for label in tqdm(fdf['label first'].unique()):\n",
    "    label_dict = {}\n",
    "    # get action level cost\n",
    "    mean, ci, true_mean = bootstrap_over_runs(fdf[fdf['label first'] == label], 'cost sum', stat_function = np.mean, show_tqdm = False)\n",
    "    label_dict['mean action level cost'] = true_mean\n",
    "    label_dict['CI action level cost'] = ci\n",
    "    # get subgoal level cost\n",
    "    mean, ci, true_mean = bootstrap_over_runs(fdf[fdf['label first'] == label], 'total_cost sum', stat_function = np.mean, show_tqdm = False)\n",
    "    label_dict['mean subgoal level cost'] = true_mean\n",
    "    label_dict['CI subgoal level cost'] = ci\n",
    "    # get success rate\n",
    "    mean, ci, true_mean = bootstrap_over_runs(fdf[fdf['label first'] == label], 'perfect last', stat_function = np.mean, show_tqdm = False)\n",
    "    label_dict['mean success rate'] = true_mean\n",
    "    label_dict['CI success rate'] = ci\n",
    "    # add to dataframe\n",
    "    agent_summary.loc[label] = label_dict\n",
    "display(agent_summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But it comes with a cost of choosing subgoals—which makes lookahead look like the most promising strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plot of action, subgoal levels costs across different agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.sample(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is mediated by the size of the problem—for smaller problem full decomp might better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_data(fdf, 'world_size item', 'cost sum', 'World Size', 'Action Level Cost', 'Action Level Cost vs World Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_data(fdf, 'world_size item', 'total_cost sum', 'World Size', 'Subgoal Level Cost\\n(states explored)', 'Subgoal Level Cost (states) vs World Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_data(fdf, 'world_size item', 'all_sequences_count sum', 'World Size', 'Subgoal Level Cost\\n(decompositions explored)', 'Subgoal Level Cost (decompositions) vs World Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_data(fdf, 'world_size item', 'perfect last', 'World Size', 'Success Rate', 'Success Rate vs World Size', y_scale='linear')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High cost avoidance ($\\lambda$) indeed leads to cheaper solutions, but also getting stuck more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_data(fdf, 'c_weight first', 'cost sum', '$\\lambda$', 'Action Level Cost', 'Action Level Cost vs $\\lambda$', x_scale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_data(fdf, 'c_weight first', 'total_cost sum', '$\\lambda$', 'Subgoal Level Cost\\n(states evaluated)', 'Subgoal Level Cost (states) vs $\\lambda$', x_scale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_data(fdf, 'c_weight first', 'all_sequences_count sum', '$\\lambda$', 'Subgoal Level Cost\\n(decompositions evaluated)', 'Subgoal Level Cost (decompositions) vs $\\lambda$', x_scale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_data(fdf, 'c_weight first', 'perfect last', '$\\lambda$', 'Success Rate', 'Success Rate vs $\\lambda$', x_scale='log', y_scale='linear')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paradoxically, avoiding action level costs leads to more subgoal selection and therefore higher costs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get world where no subgoal fails\n",
    "fdf[(fdf['label first'] == 'No Subgoals') & (fdf['perfect last'] != 1)]['world item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf[fdf['label first'] == 'No Subgoals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf[fdf['world item'] == 'Blockworld_111']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = df[(df['world'] == 'Blockworld_111') & (df['label'] == 'No Subgoals')]['_chosen_subgoal_sequence'].dropna().tail(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq[0].subgoals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = df[(df['world'] == 'Blockworld_111') & (df['label'] == 'No Subgoals') & (df['c_weight'] == 0.0)]['run_ID'].head(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['run_ID'] == id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['run_ID'] == id].tail(1)._world.item().current_state.blockmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scoping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8a0db0c320a248546b74be3a9327a6b1846905be2e5b5893711db7bb0ee00ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
