{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost of future subgoals model preference elicitation\n",
    "\n",
    "This notebook contains the code used to generate the subgoal pairs and data for analysis for the third human study.\n",
    "\n",
    "In this study, we want to see if people are sensitive to the computational costs of future subgoals. \n",
    "\n",
    "For each tower, we\n",
    "* generate a tree of subgoal decompositions\n",
    "* get the preferences over hte first subgoals across planners directly from the tree\n",
    "\n",
    "Tower generation code is taken from `Future_costs_stim_generation.ipynb`\n",
    "\n",
    "***the code in `stimuli/future_cost_stim_generation.py` is more up to date and has not yet been ported over into this notebookâ€”use this instead!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from scoping_simulations.utils.directories import PROJ_DIR, DF_DIR, STIM_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tower_generator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import p_tqdm\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import sem as sem\n",
    "\n",
    "from scoping_simulations.utils.blockworld_library import *\n",
    "from scoping_simulations.utils.blockworld import *\n",
    "\n",
    "from scoping_simulations.model.BFS_Lookahead_Agent import BFS_Lookahead_Agent\n",
    "from scoping_simulations.model.BFS_Agent import BFS_Agent\n",
    "from scoping_simulations.model.Astar_Agent import Astar_Agent\n",
    "from scoping_simulations.model.Best_First_Search_Agent import Best_First_Search_Agent\n",
    "from scoping_simulations.model.Subgoal_Planning_Agent import Subgoal_Planning_Agent\n",
    "\n",
    "from scoping_simulations.model.utils.decomposition_functions import *\n",
    "import scoping_simulations.stimuli.subgoal_tree\n",
    "import scoping_simulations.utils.blockworld_library as bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for naming the output file\n",
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we would fix the random seeds here, but the agents are being run with fixed random seeds, so this is not necessary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all columns in dataframe\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating towers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_library = bl_nonoverlapping_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = tower_generator.TowerGenerator(\n",
    "    8,\n",
    "    8,\n",
    "    block_library=block_library,\n",
    "    seed=3,\n",
    "    padding=(1, 0),\n",
    "    num_blocks=lambda: random.randint(\n",
    "        6, 18\n",
    "    ),  #  flat random interval of tower sizes (inclusive)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOWERS = 128\n",
    "towers = []\n",
    "for i in tqdm(range(NUM_TOWERS)):\n",
    "    tower = generator.generate()\n",
    "    towers.append(tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worlds = [\n",
    "    Blockworld(silhouette=t[\"bitmap\"], block_library=bl.bl_nonoverlapping_simple)\n",
    "    for t in towers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate subgoal decompositon tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = worlds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposer = Rectangular_Keyholes(\n",
    "    sequence_length=MAX_LENGTH,\n",
    "    necessary_conditions=[\n",
    "        Area_larger_than(area=1),\n",
    "        # Area_smaller_than(area=30), # used to be 21\n",
    "        Mass_smaller_than(area=18),\n",
    "        No_edge_rows_or_columns(),\n",
    "    ],\n",
    "    necessary_sequence_conditions=[\n",
    "        Complete(),\n",
    "        No_overlap(),\n",
    "        Supported(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sga = Subgoal_Planning_Agent(\n",
    "    lower_agent=Best_First_Search_Agent(), decomposer=decomposer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sga.set_world(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg_tree = sga.get_subgoal_tree(only_solved_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, all_sequences, solved_sequences = sga.plan_subgoals(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_of_tree(tree):\n",
    "    if type(tree) == stimuli.subgoal_tree.SubgoalTree:\n",
    "        tree = tree.root\n",
    "    return 1 + sum([size_of_tree(child) for child in tree.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_of_tree(sg_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot all the children in a combined plot\n",
    "# n_children = len(sg_tree.root.children)\n",
    "# # make empty fig\n",
    "# fig, axs = plt.subplots(1, n_children, figsize=(n_children*4,4))\n",
    "# # plot each child\n",
    "# for i, child in enumerate(sg_tree.root.children):\n",
    "#     child.subgoal.visualize(title=i, ax = axs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s.V() for s in solved_sequences if s.solution_cost()].count(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequences of different length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the tree to generate sequences of subgoals up to a certain length\n",
    "2. Calculate V for each sequence from C, reward\\\n",
    "    What do we do about `c_weight`?\n",
    "3. Over all sequences of a length, get list of V's for the first subgoal\n",
    "4. Use the list of V's to calculate preferences over the first subgoals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOFTMAX_K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgoal_choice_preferences(solved_sequences, c_weight=None):\n",
    "    \"\"\"Get a dict with choice prefernece for each initial subgoal of the form:\n",
    "    {subgoal: [preference for the ith depth agent]}\n",
    "    Set lambda in the agent itself\"\"\"\n",
    "    # generate subsequences\n",
    "    length_sequences = {}\n",
    "    for length in list(range(1, MAX_LENGTH + 1)):\n",
    "        length_sequences[length] = []\n",
    "        for (\n",
    "            seq\n",
    "        ) in (\n",
    "            solved_sequences\n",
    "        ):  # needs to be solved sequences to ensure that they're all solvable and result in the full decompositon (make sure the proper flag is set above)\n",
    "            if len(seq) <= length:\n",
    "                length_sequences[length].append(seq)\n",
    "            elif len(seq) > length:\n",
    "                # generate a truncated sequence\n",
    "                shortenend_seq = Subgoal_sequence(seq.subgoals[0:length])\n",
    "                length_sequences[length].append(shortenend_seq)\n",
    "        # clear out duplicates according to subgoals\n",
    "        seen = set()\n",
    "        length_sequences[length] = [\n",
    "            x\n",
    "            for x in length_sequences[length]\n",
    "            if not (x.names() in seen or seen.add(x.names()))\n",
    "        ]  # I assume that a tuple of the same objects is the same even when recreated\n",
    "\n",
    "    subgoals = {}\n",
    "    # get first subgoal V's\n",
    "    subgoal_depth_Vs = {}\n",
    "    for depth in length_sequences:\n",
    "        subgoal_depth_Vs[depth] = {}\n",
    "        for seq in length_sequences[depth]:\n",
    "            V = seq.V(c_weight) if c_weight is not None else seq.V()\n",
    "            if seq.subgoals[0].name in subgoal_depth_Vs[depth]:\n",
    "                subgoal_depth_Vs[depth][seq.subgoals[0].name] += [V]\n",
    "            else:\n",
    "                subgoal_depth_Vs[depth][seq.subgoals[0].name] = [V]\n",
    "            if seq.subgoals[0].name not in subgoals:\n",
    "                subgoals[seq.subgoals[0].name] = seq.subgoals[0]\n",
    "\n",
    "    # get list of preferences for depth per subgoal\n",
    "    subgoal_preferences = {}\n",
    "    for subgoal_name in subgoals.keys():\n",
    "        subgoal_preferences[subgoal_name] = {}\n",
    "        for depth in length_sequences:\n",
    "            # get subgoal preference for depth\n",
    "            # using softmax with K defined above\n",
    "            total_best_Vs = [max(vs) for vs in subgoal_depth_Vs[depth].values()]\n",
    "            sg_V = max(subgoal_depth_Vs[depth][subgoal_name])\n",
    "            softmax_val = math.exp(SOFTMAX_K * sg_V) / sum(\n",
    "                [math.exp(SOFTMAX_K * v) for v in total_best_Vs]\n",
    "            )\n",
    "            subgoal_preferences[subgoal_name][depth] = softmax_val\n",
    "    return subgoal_preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgoal_choice_preferences_over_lambda(solved_sequences, lambdas):\n",
    "    \"\"\"Generates dict with {$\\lambda$: {subgoal: [preference for the ith depth agent]}}\"\"\"\n",
    "    subgoal_preferences_over_lambda = {}\n",
    "    for l in lambdas:\n",
    "        subgoal_preferences_over_lambda[l] = get_subgoal_choice_preferences(\n",
    "            solved_sequences, l\n",
    "        )\n",
    "    return subgoal_preferences_over_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_subgoal_choice_preferences = get_subgoal_choice_preferences_over_lambda(\n",
    "    solved_sequences, np.linspace(0, 1, 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the evolution of choice preferences\n",
    "num_subgoals = len(list(l_subgoal_choice_preferences.values())[0].keys())\n",
    "fig, axes = plt.subplots(1, num_subgoals, figsize=(num_subgoals * 4, 4))\n",
    "for l in l_subgoal_choice_preferences.keys():\n",
    "    for i, subgoal_name in enumerate(l_subgoal_choice_preferences[l].keys()):\n",
    "        for depth in l_subgoal_choice_preferences[l][subgoal_name].keys():\n",
    "            # color the dots according to depth\n",
    "            color = plt.cm.viridis(depth / MAX_LENGTH)\n",
    "            axes[i].scatter(\n",
    "                l,\n",
    "                l_subgoal_choice_preferences[l][subgoal_name][depth],\n",
    "                label=depth,\n",
    "                color=color,\n",
    "            )\n",
    "        axes[i].set_title(subgoal_name)\n",
    "        axes[i].set_xlabel(\"$\\lambda$\")\n",
    "        axes[i].set_ylabel(\"Preference\")\n",
    "        # remove duplicate labels\n",
    "        handles, labels = axes[i].get_legend_handles_labels()\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        axes[i].legend(by_label.values(), by_label.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to marginalize over lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_marginalized_subgoal_choice_preferences_over_lambda(solved_sequences, lambdas):\n",
    "    subgoal_preferences_over_lambda = get_subgoal_choice_preferences_over_lambda(\n",
    "        solved_sequences, lambdas\n",
    "    )\n",
    "    # marginalize over lambda\n",
    "    subgoal_preferences = {}\n",
    "    for subgoal_name in subgoal_preferences_over_lambda[lambdas[0]].keys():\n",
    "        subgoal_preferences[subgoal_name] = {}\n",
    "        for depth in subgoal_preferences_over_lambda[lambdas[0]][subgoal_name].keys():\n",
    "            subgoal_preferences[subgoal_name][depth] = np.mean(\n",
    "                [\n",
    "                    subgoal_preferences_over_lambda[l][subgoal_name][depth]\n",
    "                    for l in lambdas\n",
    "                ]\n",
    "            )\n",
    "    return subgoal_preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subgoal_choice_preferences_over_lambda(solved_sequences, np.linspace(0, 1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot as barplot\n",
    "subgoal_preferences = get_marginalized_subgoal_choice_preferences_over_lambda(\n",
    "    solved_sequences, np.linspace(0.1, 1, 100)\n",
    ")\n",
    "num_subgoals = len(subgoal_preferences.keys())\n",
    "fig, axes = plt.subplots(1, num_subgoals, figsize=(num_subgoals * 4, 4))\n",
    "for i, subgoal_name in enumerate(subgoal_preferences.keys()):\n",
    "    for depth in subgoal_preferences[subgoal_name].keys():\n",
    "        # color the dots according to depth\n",
    "        color = plt.cm.viridis(depth / MAX_LENGTH)\n",
    "        axes[i].bar(\n",
    "            depth, subgoal_preferences[subgoal_name][depth], label=depth, color=color\n",
    "        )\n",
    "    axes[i].set_title(subgoal_name)\n",
    "    axes[i].set_xlabel(\"Depth\")\n",
    "    axes[i].set_ylabel(\"Preference\")\n",
    "    # remove duplicate labels\n",
    "    handles, labels = axes[i].get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    axes[i].legend(by_label.values(), by_label.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us the absolute choice preference of the planner. We also want the relative choice preference, which is the ratio in entropy of the distribution over the first subgoals with and without the planner included. The higher the difference, the more the planner is preferred. This indicates the relative to the entropy of the other planners introducing the new one reduces entropy by a certain amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return -sum([p_i * math.log(p_i) for p_i in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_subgoal_informativity(subgoal_preferences):\n",
    "    \"\"\"Returns dict with {subgoal: informativeness of subgoal}\"\"\"\n",
    "    subgoal_relative_preferences = {}\n",
    "    for subgoal_name in subgoal_preferences.keys():\n",
    "        subgoal_relative_preferences[subgoal_name] = {}\n",
    "        entropy_all = entropy(subgoal_preferences[subgoal_name].values())\n",
    "        for depth in subgoal_preferences[subgoal_name].keys():\n",
    "            other_entropy = entropy(\n",
    "                [\n",
    "                    subgoal_preferences[subgoal_name][d]\n",
    "                    for d in subgoal_preferences[subgoal_name].keys()\n",
    "                    if d != depth\n",
    "                ]\n",
    "            )\n",
    "            subgoal_relative_preferences[subgoal_name][depth] = (\n",
    "                entropy_all / other_entropy\n",
    "            )\n",
    "        subgoal_relative_preferences\n",
    "    return subgoal_relative_preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_subgoal_choice_preferences(subgoal_preferences):\n",
    "    \"\"\"Returns dict with {subgoal: [relative preference for the ith depth agent]}\"\"\"\n",
    "    subgoal_relative_preferences = {}\n",
    "    for subgoal_name in subgoal_preferences.keys():\n",
    "        subgoal_relative_preferences[subgoal_name] = {}\n",
    "        for depth in subgoal_preferences[subgoal_name].keys():\n",
    "            # best - minus second best\n",
    "\n",
    "            subgoal_relative_preferences[subgoal_name][depth] = (\n",
    "                entropy_all / other_entropy\n",
    "            )\n",
    "    return subgoal_relative_preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_subgoal_preferences = get_relative_subgoal_choice_preferences(\n",
    "    subgoal_preferences\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot relative as barplot\n",
    "num_subgoals = len(relative_subgoal_preferences.keys())\n",
    "fig, axes = plt.subplots(1, num_subgoals, figsize=(num_subgoals * 4, 4))\n",
    "for i, subgoal_name in enumerate(relative_subgoal_preferences.keys()):\n",
    "    for depth in relative_subgoal_preferences[subgoal_name].keys():\n",
    "        # color the dots according to depth\n",
    "        color = plt.cm.viridis(depth / MAX_LENGTH)\n",
    "        axes[i].bar(\n",
    "            depth,\n",
    "            relative_subgoal_preferences[subgoal_name][depth],\n",
    "            label=depth,\n",
    "            color=color,\n",
    "        )\n",
    "    axes[i].set_title(subgoal_name)\n",
    "    axes[i].set_xlabel(\"Depth\")\n",
    "    axes[i].set_ylabel(\"Relative Preference\")\n",
    "    # remove duplicate labels\n",
    "    handles, labels = axes[i].get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    axes[i].legend(by_label.values(), by_label.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgoal_preferences = get_marginalized_subgoal_choice_preferences_over_lambda(\n",
    "    solved_sequences, np.linspace(0.1, 1, 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgoal_preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets put everything into a big dataframe\n",
    "initial_subgoals_df = pd.DataFrame.from_dict(subgoal_preferences, orient=\"index\")\n",
    "# add in absolute in col names\n",
    "initial_subgoals_df.columns = [str(col) + \"_abs\" for col in initial_subgoals_df.columns]\n",
    "# add in relative preferences\n",
    "relative_subgoal_preferences = get_relative_subgoal_choice_preferences(\n",
    "    subgoal_preferences\n",
    ")\n",
    "relative_subgoals_df = pd.DataFrame.from_dict(\n",
    "    relative_subgoal_preferences, orient=\"index\"\n",
    ")\n",
    "# add in relative in col names\n",
    "relative_subgoals_df.columns = [\n",
    "    str(col) + \"_rel\" for col in relative_subgoals_df.columns\n",
    "]\n",
    "# merge\n",
    "initial_subgoals_df = pd.merge(\n",
    "    initial_subgoals_df, relative_subgoals_df, left_index=True, right_index=True\n",
    ")\n",
    "# add in subgoalts themselves\n",
    "# add the current world index\n",
    "initial_subgoals_df[\"world\"] = world_index\n",
    "# we need to recover them from the solved_sequences\n",
    "subgoals = []\n",
    "for sequence in solved_sequences:\n",
    "    if sequence.subgoals[0].name not in subgoals:\n",
    "        subgoals.append(sequence.subgoals[0])\n",
    "# add in according to subgoal name\n",
    "subgoals_df = pd.DataFrame.from_dict(\n",
    "    {subgoal.name: subgoal for subgoal in subgoals}, orient=\"index\", columns=[\"subgoal\"]\n",
    ")\n",
    "# merge with initial_subgoals_df\n",
    "initial_subgoals_df = pd.merge(\n",
    "    initial_subgoals_df, subgoals_df, left_index=True, right_index=True\n",
    ")\n",
    "# add in additional subgoal info\n",
    "initial_subgoals_df[\"C\"] = initial_subgoals_df[\"subgoal\"].apply(lambda x: x.C)\n",
    "initial_subgoals_df[\"R\"] = initial_subgoals_df[\"subgoal\"].apply(lambda x: x.R())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_subgoals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out initial_subgoals_df\n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "initial_subgoals_df.to_csv(\"initial_subgoals_df_\" + time_stamp + \".csv\")\n",
    "# save the worlds\n",
    "with open(\"worlds_\" + time_stamp + \".pkl\", \"wb\") as f:\n",
    "    pickle.dump(worlds, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('scoping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8a0db0c320a248546b74be3a9327a6b1846905be2e5b5893711db7bb0ee00ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
